% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[review]{cvpr}
%\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage[sorting=none]{biblatex}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[ruled,vlined]{algorithm2e}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}


%%%%%%%%% TITLE

\title{QuMaDe: \underline{Qu}ick Foreground \underline{Ma}sk and Monocular \underline{De}pth Data Generation
%\title{SmallDepthMask: Quick Generation of Large Dataset for Monocular Depth Estimation and Foreground Segmentation from few Internet Images
\includegraphics[width=1\textwidth]{samplerecord.png}
}

\author{Sridevi Bonthu\\
Vishnu Institute of Technology\\
Bhimavaram, AP, India 534202\\
{\tt\small sridevi.b@vishnu.edu.in}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Abhinav Dayal\\
Vishnu Institute of Technology\\
Bhimavaram, AP, India 534202\\
{\tt\small abhinav.dayal@vishnu.edu.in}
\and
Sumit Gupta\\
Vishnu Institute of Technology\\
Bhimavaram, AP, India 534202\\
{\tt\small sumit.gupta@vishnu.edu.in}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
  Segmentation of the desired object along with depth estimation is useful in various applications like robotics and autonomous navigation. Any deep learning workflow to estimate monocular depth and segment the desired foreground object in a scene require significant training data. The data generation process usually involve expensive hardware like RGB-D sensors, Laser Scanners or significant manual involvement. Moreover, for every specific foreground object, the data collection process need to repeat. This paper presents a novel way to utilize only a small number of readily available png images with transparency for the foreground object, and representative background images from the internet and combine them to generate a large dataset for deep learning, utilizing current state of the art monocular depth estimation techniques. To illustrate the effectiveness of the data generation approach, this paper presents a baseline model for depth and foreground mask estimation for detecting cattle on road using the generated data from the proposed approach. The baseline model exhibits strong generalization to real scenarios. The generated dataset is available for public use.
\end{abstract}

%Keywords : Large Dataset, Depth image, Segmentation Mask, Depth prediction, Custom Dataset, Baseline model.
 

%%%%%%%%% BODY TEXT
\section{Introduction}
Depth estimation and segmentation of desired objects in the scene are often used together in many vision tasks~\cite{lin2018joint}, like autonomous navigation of agents, augmented reality, self driving cars and other robotics applications. In all these applications, identification of desired objects precisely in the scene and its depth estimate from the camera are crucial for safe and effective navigation. Modern RGB-D sensors like OAK-D\footnote{OpenCV AI Kit: \url{https://opencv.org/introducing-oak-spatial-ai-powered-by-opencv/}} are capable of simultaneously running advanced neural networks while providing depth from two stereo cameras and color information from a single 4K camera in the center. Deep learning based techniques using convolution neural nets have effective solution in both depth estimation and semantic segmentation. In general for high accuracy outcomes, a deep learning network is dependent on large training dataset availability. To gather such data itself incurs high cost and time. For specific applications requiring several foreground objects against variety of backgrounds become even more challenging in terms of simulating those scenarios. Synthetic datasets using Virtual Reality have been proposed to that end~\cite{ros2016synthia}.

Recent research indicates effective use of readily available images on the internet to curate training data~\cite{li2018megadepth}. This paper introduces \textit{QuMaDe}, a way to curate custom dataset containing hundreds of thousands to millions of images by multiplexing desired foreground objects over representative background scenes, while also generating corresponding depth and foreground mask images. This significantly reduces the cost and time overheads. The authors also experiment by creating baseline models for several application contexts and show that the generated data successfully generalizes to detect relevant objects in real scenes. Multiplexing, combined with random cropping, scaling and translation, makes the data generation fast and effective. With only $100$ pairs of background and foreground images, the authors generate $0.4$ million triplets of foreground over background, monocular depth and foreground mask images by effectively leveraging existing SOTA models for depth estimation.

The main contributions of this paper are the following:
\begin{enumerate}
\item A novel effort to mix and match foreground and background images reducing the need for complex scene generation for data curation.
\item Curate large dataset to effectively train models to detect depth and mask for specific foreground objects over any target background, from a limited input of ready available internet images.
\item Combine image, depth map and foreground mask in a single dataset using current SOTA models for depth estimation.
\item To release curated dataset and the trained models making them publicly available. Researchers can use this single dataset to do segmentation, train models to predict depth, or to predict both depth and mask.
\end{enumerate}
  

The title figure represents an example from the generated dataset to help detect cattle on road,  a common happening on the Indian roads that leads to several accidents each year involving loss of life and property. The generated dataset, \textit{MODES}\footnote{Curated Dataset for Cattle on Road: \url{https://www.kaggle.com/bsridevi/modes-dataset-of-stray-animals}} and the trained model are publicly available


\section {Related Work}

A depth image is an image channel in which each pixel relates to a distance between the image plane and the corresponding object in the RGB image. Monocular depth gives information about depth and distance and Monocular Depth Estimation is the task of estimating scene depth using a single image\cite{abuolaim2020defocus}. Image Segmentation is the process of partitioning an image into multiple segments and it can be used for locating objects and boundaries~\cite{amza2012review}. RGBD image is a combination of a RGB image and its corresponding depth image\cite{zhang2018deep}.
 
Depth information is integral to many problems in robotics including mapping, localization and obstacle avoidance for terrestrial and aerial vehicles, autonomous navigation, and in computer vision, including augmented and virtual reality\cite{marchand2015pose}. RGBD datasets usually collected using depth sensors, monocular cameras and LiDAR scanners are expensive and data collection is a time consuming job. The well known datasets for monocular 3D object detection are Context-Aware MixEd ReAlity (CAMERA), Objectron, Kitty3D, Cityscape3D, Synthia, etc. and these datasets have limitations like indoor only images, small number of training examples and sparse sampling. Some of the frequently used RGBD datasets are the Kitti dataset~\cite{geiger2013vision}, the Synthia dataset~\cite{ros2016synthia}, Make3D dataset~\cite{saxena2008make3d} and NYU dataset~\cite{silberman2012indoor}

The dataset Kiiti~\cite{geiger2013vision} is collected using a vehicle equipped with a sparse Velodyne VLP-64 LiDAR scanner and RGB cameras, and features street scenes in and around the German city of Karlsruhe. The Primary application of this dataset involves perception tasks in the context of self-driving. Synthia~\cite{ros2016synthia} is a street scene dataset with depth maps of synthetic data, requiring domain adaptation to apply to real world settings. Cityscapes~\cite{cordts2016cityscapes} provides a dataset of street scenes, albeit with more diversity than KITTI. Sintel~\cite{mayer2016large} is another synthetic dataset which mainly comprises of outdoors scenes.

Megadepth~\cite{li2018megadepth} is a large-scale dataset of outdoor images collected from internet, with depth maps reconstructed using structure-from-motion techniques, but this dataset lacks in ground truth depth and scale. The RedWeb~\cite{xian2018monocular} dataset provide depth maps generated from stereo images which are freely available in large-scale data platforms such as Flicker. The datasets MegaDepth and RedWeb can be easily computed with the existing MVS methods.

Make3D~\cite{saxena2008make3d} provides RGB and depth information for outdoor scenes. The NYUv2 dataset~\cite{silberman2012indoor} is widely used for monocular depth estimation in indoor environments. The data was collected with a Kinect RGBD camera, which provides sparse and noisy depth returns. These returns are generally in-painted and smoothed before they are used for monocular depth estimation tasks. As a result, while the dataset includes sufficient samples to train modern machine learning pipelines, the ``ground-truth" depth does not necessarily correspond to true scene depth.

Most of the existing datasets consists of indoor images, or outdoor images of city streets. For every specific application, like detecting animals roaming on roads for self driving or assisted driving cars, or people inside a room for autonomous room cleaners etc. researchers need to curate specific dataset to train relevant deep learning models. This paper proposes a technique to come up with a custom dataset by using existing accurate depth predictor models, like High Quality Monocular Depth Estimation via Transfer Learning(nyu.h5)~\cite{alhashim2018high} making the task of curating dataset extremely simple and cost effective.

\begin{figure*}
  \begin{center}
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=1.0\linewidth]{bgimages.jpg}
  \end{tabular}
  \begin{tabular}{@{}c@{}}
      \includegraphics[width=1.0\linewidth]{fgimages.jpg}
  \end{tabular}
  \end{center}
  \caption{Scene and foreground object images}
  \label{fig:sceneandfg}
\end{figure*}

\IncMargin{1em}
\begin{algorithm*}
  \label{algo:datagen}
  \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
  \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{$m$ Background Image paths, $2n$ Foreground Image paths, multiplexing factor $k$, batch size $b$ \emph{must be multiple of $k$}}
  \Output{Yield $2kmn$ fg-bg, mask and depth images in batches of size $b$}
  \BlankLine
  \emph{for offline use it creates 3 folders with fg\_bg, mask and depth each having $2kmn$ images}\;

  \For{$bg\leftarrow 1$ \KwTo $m$}{
    \For{$fg\leftarrow 1$ \KwTo $2n$}{
      \For{$i\leftarrow 1$ \KwTo $k$}{
        $croppedbg\leftarrow$ take maximal random crop of $448\times448$ from $bg$ without affecting the aspect ratio\;
        randomly pick a center point $(x, y)$ in range $[0,447]$\;
     	  randomly pick a scale in range $[0.3, 0.6]$ (ratio of area $fg$ covers $bg$)\;
     	  create $fg-bg$ image by resizing the $fg$ to scale and place it on top of $croppedbg$ centered at $x, y$ calculated\;
        calculate binary mask from current placement of $fg$ by thresholding the transparency channel.
        save $fg-bg$ image and mask
        add $fg-bg$ image to a batch\;
    }
    \If{$b$ new fg-bg images generated}{
      run depth model on batch and save corresponding depth images.
    }
    }}
\caption{Generate Dataset(\textit{[bgimages]}, \textit{[fgimages]}, $k$, $b$)}
\end{algorithm*}


\begin{figure*}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{good.png}
  \end{center}
  \caption{Three image sets resulted from the algorithm used. (top) A scene image on which a foreground object is positioned at random location with random scale, (middle) respective mask for the foreground, and (bottom) calculated depth by using a model.}
  \label{fig:finaldataset}
\end{figure*}

\section{Method}
The curated dataset must have following objectives:
\begin{enumerate}
\item It should always include the foreground object. 
\item It should drive deep learning models that generalize well.
\item It should provide accurate dense depth maps in line with SOTA models.
\item It should provide accurate foreground mask.
\item It should be able to generate the data online during training phase dynamically.
\end{enumerate}

\subsection{Data Acquisition}
The first step to curate data is to determine a target application scenario and thus determine the foreground object(s) and the representative background context. At the same time the dataset must have sufficient variability to include majority of the types and views that the trained deep network model may see when deployed.

We propose to download or take RGB image of $n$ foreground object(s) and $m$ background images (we used $n=m=100$) balancing the types and views. For example, for  \textit{MODES} dataset, we chose several cow, bull and calf types, individual  or in group, sitting, standing or walking, and from various angles. Similarly for background, we chose backgrounds of streets,  storefronts, main roads, highways, markets, railway tracks, landscapes, garbage piles etc. PNG images with transparency are readily available on the internet for almost any desired foreground object. Such images will easily allow to generate foreground mask from non-transparent pixels. If not, tools like GIMP~\cite{howat2014greenland}, combined with deep learning foreground extractors\footnote{\url{https://www.remove.bg/} uses a combination of Image based techniques and DNN to separate foreground from background} can help generate the required PNG foreground images. Figure~\ref{fig:sceneandfg} shows few of the sample scene and foreground images used for the creation of this dataset. 

\subsection{Multiplexing and Depth Generation}

This step is to place each foreground object several times on to the background images generating an fg-bg image and the foreground mask corresponding to the foreground placement and scale. Depth is computed from the fg-bg image via the model proposed by Ibraheem Alhashim et al. in their paper titled ``High Quality Monocular Depth Estimation via Transfer Learning"~\cite{alhashim2018high}\footnote{source code for depth estimation model: \url{https://github.com/ialhashim/DenseDepth/blob/master/DenseDepth.ipynb}}. This model takes $448\times448$ size images as input, hence we resize all background images to this size while maintaining their aspect ratio.

The data generation process is completely online and produces one batch of images for training a deep model. By randomly repeating one foreground object $k$ times at varied locations and scales for each background image and repeating another $k$ times with horizontally flipped version of the same foreground, one can generate $2kmn$ fg-bg images. In addition a random crop from background image instead of a fixed initial crop from the source image can add to more variability in the input. For $k=20$ and $n=m=100$ this becomes $400,000$ fg-bg images. Algorithm \ref{algo:datagen} describes the data generation process


\section{Experimental Analysis}
In this section, we provide a baseline for monocular depth estimation and foreground segmentation on the generated \textit{MODES} dataset. Convolutional Neural Networks(CNN) are progressive in exploring structural features and spatial image formation. To come up with baseline, we started  training simple CNN, Resnet, and Unet++. The state-of-the-art models for image segmentation are variants of U-Net and fully convolutional networks (FCN)\cite{drozdzal2016importance}. Long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling \cite{zhou2019unet++}. Short skip connections can be used to build deep FCNs. 

\begin{figure}
  \centering
  \includegraphics[width=1.0\linewidth]{networkarchitecture.jpg}
  \caption{Network Architecture}
  \label{fig:modelarch}
\end{figure}

\begin{figure*}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{validinference.png}
  \end{center}
  \caption{Foreground mask and depth inference on validation data}
  \label{fig:validinfer}
\end{figure*}


\begin{figure*}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{unseeninference.png}
  \end{center}
  \caption{Foreground mask and depth inference on real life unseen data.}
  \label{fig:unseeninfer}
\end{figure*}

\subsection{Model}
By using both long and short skip connections we proposed a light weight model following U-Net architecture with two decoder networks meant for foreground mask prediction and depth prediction. The architecture of the model is shown in Figure \ref{fig:modelarch}. The total number of parameters of this model are $5,525,568$ including both the decoders. 

The encoder part of the network is comprised of four downsampling units. Every downsampling unit compresses the input scene image with the help of a series of convolutional operations. In our implementation, the source image of size $128\times128$, changed into $64\times64 \rightarrow 32\times32 \rightarrow  16\times16 \rightarrow 8\times8$. This model has DepthDecoder and MaskDecoder and each of them is comprised of four upsampling units. The compressed source image is expanded with the help of Atrous and Transposed convolution operations. The encoder outcome $8\times8$ is expanded into $16\times16 \rightarrow 32\times32 \rightarrow 64\times64 \rightarrow 128\times128$. As shown in model architecture Figure~\ref{fig:modelarch} the outcome of encoder downsampling units were added to the outcomes of decoder upsampling units.

We have trained this model on the entire \textit{MODES} dataset from scratch with train-test-split of $70-30\%$. During training, the network is trained with the batch size of $64$ for $10$ epochs using SGD optimizer~\cite{bottou2010large}. Every epoch took one hour of time on GPU because of the huge training data. We have used OneCycleLR scheduler~\cite{smith2018disciplined} with a maximum Learning rate of $0.1$. This made the initial learning rate as $0.0099$. The Deep Convolutional Neural Networks encoder is fed with an image $(128\times128)$ and the first decoder outputs a mask image and the second decoder outputs a depth image. To reduce overfitting~\cite{perez2017effectiveness}, and achieve generalization this work employed the augmentation techniques of Random Rotation, Random Grayscale, Color Jitter, random horizontal flips and random channel swaps. 



\begin{figure*}
  \begin{center}
  \includegraphics[width=1.0\textwidth]{refinement.png}
  \end{center}
  \caption{Using Semantic Segmentation for adaptive placement and scaling. (Top) semantic segmentation. (Middle) Example of poor scaling and placement in proposed approach. (Bottom) Attempted correct placement and scaling}
  \label{fig:refinement}
\end{figure*}

\begin{figure}[h!]
  \begin{center}
  \includegraphics[width=0.8\linewidth]{occlusion.jpg}
  \end{center}
  \caption{Restoring occlusion from semantic segmentation}
  \label{fig:occlusion}
\end{figure}

\subsection{Loss function}
Deciding a universal loss function is not possible for complex objectives like foreground segmentation and depth prediction. Based on the survey done by Shruti Jadon~\cite{jadon2020survey} we have picked L1 loss and SSIM(Structural Similarity Index) loss~\cite{zhao2015loss}. Their work also suggested to use a penalty term, which helps the network to focus towards hard-to-segment boundary regions. The Loss is calculated with the help of L1 and SSIM at both the decoders and employed regularization for weight penalty.

For training our network with two decoders, we defined the same loss function $L$ for depth and mask prediction, between $y$ and $\hat{y}$ as the weighted sum of two loss function values.

\begin{equation}
L(y, \hat{y}) = \lambda L_{term1}(y, \hat{y}) + (1 - \lambda) L_{term2}(y, \hat{y})
\end{equation}

The first loss term $L_{term1}(y, \hat{y})$ is the point-wise L1 loss defined on the predictions of Mask Decoder and  Depth Decoder units of the network.

\begin{equation}
L_{term1}(y, \hat{y}) = \frac{1}{n} \sum_{x=1}^{n} \mid y_i - \hat{y_i} \mid
\end{equation}

The second loss term $L_{term2}(y, \hat{y})$ uses a commonly used metric for image reconstruction task i.e., SSIM. Many recent depth prediction CNNs employed this metric. The loss term is redefined as shown in equation as SSIM has an upper bound of one.

\begin{equation}
L_{term1}(y, \hat{y}) = \frac{1 - SSIM(y, \hat{y})}{2}
\end{equation}

Different weight parameters $\lambda$ were tried and we have ended with a value $\lambda = 0.84$. The final loss function is as follows.

\begin{equation}
L(y, \hat{y}) = 0.84 \ast L_{term1}(y, \hat{y}) + 0.16 \ast L_{term2}(y, \hat{y})
\end{equation}


\subsection{Results}
The model is trained on the entire dataset and obtained significant accuracy and minimal loss. The outcome of the model on validation dataset is shown in Figure \ref{fig:validinfer}. and on the unseen data is shown in Figure \ref{fig:unseeninfer}. The unseen data fed to the model is a real picture and not one curated as in \textit{QuMaDe} where foreground is placed on background. From the obtained depths and foreground masks, it is very clear that the model generalized well. Few exceptions like the spots on cow and two calves not having very good detection indicate the non-presence of such examples in the training set. However, it should be straightforward to introduce few more examples to make the application more robust.



\section{Conclusion and Future Work}

We presented a novel approach to generate a robust large dataset for depth and foreground mask estimation, where the dataset creation cost is kept low by intelligently multiplexing few foreground objects and high quality scene images collected from the Internet. We demonstrated the use of generated data to predict depth and mask for cattle on road. The light-weight baseline model presented, generalizes well to real life unseen data.  We also demonstrate successful use of State-of-the-art models like DenseDepth to generate depth images for the curated foreground-background combined images. In addition we release the $400K$ multiplexed images, depth, and masks in \textit{MODES} data for public use.

Due to random scaling and placement the fg-bg images are often not so realistic. Figure \ref{fig:refinement} middle show sample images with wrong placement and scale. Regardless of this, the trained model is generalizing well. However, we experimented with detection of ground and sky regions from semantic segmentation by following DeepLab architecture\cite{deeplabv3plus2018}. That combined with the depth information of the background gives necessary cues as where to place the foreground image and at what scale. Figure \ref{fig:refinement} shows the outcomes. We further experimented with adding occlusions from semantic segmentation information where non ground/sky pixels that belong to regions starting below the top margin of the foreground location, are placed above the foreground. Figure \ref{fig:occlusion} illustrates this. The effect of such data on training is yet to be explored. At the same time, the generated images by such informed scaling and placement are also not free from artifacts. The orientation and unknown size of foreground object pose a challenge, leading to use of several experimental constants in the transformation process. We would like to formalize that and analyze its outcome on training as future work.

\section{Acknowledgement}
Authors are grateful to Vishnu Institute of Technology for providing necessary infrastructure, and to Rohan Shravan of The School of AI for initiating the idea and providing necessary support to carry out the research.

{\small
\bibliographystyle{unsrt}
\bibliography{references}
}

\end{document}